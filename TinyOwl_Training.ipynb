{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü¶â TinyOwl Fine-Tuning Notebook\n",
        "\n",
        "**Train TinyLlama on your 373K theological chunks**\n",
        "\n",
        "---\n",
        "\n",
        "## What This Does\n",
        "\n",
        "This notebook fine-tunes TinyLlama-1.1B on your complete theological database:\n",
        "- **Phase 1**: Domain adaptation (learn theological knowledge)\n",
        "- **Phase 2**: Instruction tuning (learn to answer questions)\n",
        "\n",
        "**Total training time**: ~12-20 hours on free T4 GPU\n",
        "\n",
        "---\n",
        "\n",
        "## Before You Start\n",
        "\n",
        "### ‚úÖ Required Files (upload to Colab):\n",
        "1. `domain_adaptation.jsonl` (281 MB) - Your 373K chunks\n",
        "2. `instruction_tuning.jsonl` (~10-15 MB) - Q&A pairs\n",
        "\n",
        "### ‚úÖ GPU Setup:\n",
        "- Go to **Runtime** ‚Üí **Change runtime type**\n",
        "- Select **T4 GPU** (free tier)\n",
        "- Click **Save**\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. **Upload your files** (left sidebar ‚Üí folder icon)\n",
        "2. **Run each cell in order** (click play button or Shift+Enter)\n",
        "3. **Wait for training** (~12-20 hours total)\n",
        "4. **Download your model** when complete\n",
        "\n",
        "Let's go! üöÄ"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "Installing Unsloth (optimized training library) and required packages.\n",
        "\n",
        "**Time**: ~2-3 minutes"
      ],
      "metadata": {
        "id": "step1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install Unsloth for efficient QLoRA training\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 2: Check GPU & Upload Files\n",
        "\n",
        "Verify GPU is available and guide file upload."
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
        "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU found!\")\n",
        "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí Select T4 GPU\")\n",
        "    raise SystemExit\n",
        "\n",
        "# Check for uploaded files\n",
        "print(\"\\nüìÅ Checking for training files...\")\n",
        "\n",
        "domain_file = Path(\"/content/domain_adaptation.jsonl\")\n",
        "qa_file = Path(\"/content/instruction_tuning.jsonl\")\n",
        "\n",
        "if domain_file.exists():\n",
        "    print(f\"‚úÖ domain_adaptation.jsonl found ({domain_file.stat().st_size / 1024**2:.1f} MB)\")\n",
        "else:\n",
        "    print(\"‚ùå domain_adaptation.jsonl NOT FOUND\")\n",
        "    print(\"   ‚Üí Upload it using the folder icon on the left\")\n",
        "\n",
        "if qa_file.exists():\n",
        "    print(f\"‚úÖ instruction_tuning.jsonl found ({qa_file.stat().st_size / 1024**2:.1f} MB)\")\n",
        "else:\n",
        "    print(\"‚ùå instruction_tuning.jsonl NOT FOUND\")\n",
        "    print(\"   ‚Üí Upload it using the folder icon on the left\")\n",
        "\n",
        "if not (domain_file.exists() and qa_file.exists()):\n",
        "    print(\"\\n‚ö†Ô∏è  Upload both files before continuing!\")\n",
        "else:\n",
        "    print(\"\\nüéâ All files ready! Proceed to next step.\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 3: Load Training Data\n",
        "\n",
        "Loading your theological datasets."
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "def load_jsonl(filepath):\n",
        "    \"\"\"Load JSONL file\"\"\"\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "print(\"üìö Loading domain adaptation dataset...\")\n",
        "domain_data = load_jsonl(\"/content/domain_adaptation.jsonl\")\n",
        "print(f\"‚úÖ Loaded {len(domain_data):,} theological chunks\")\n",
        "\n",
        "print(\"\\nüìö Loading instruction tuning dataset...\")\n",
        "qa_data = load_jsonl(\"/content/instruction_tuning.jsonl\")\n",
        "print(f\"‚úÖ Loaded {len(qa_data):,} Q&A pairs\")\n",
        "\n",
        "print(\"\\nüìä Sample domain chunk:\")\n",
        "print(f\"   {domain_data[0]['text'][:200]}...\")\n",
        "\n",
        "print(\"\\nüìä Sample Q&A pair:\")\n",
        "print(f\"   Q: {qa_data[0]['instruction']}\")\n",
        "print(f\"   A: {qa_data[0]['output'][:100]}...\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## PHASE 1: Domain Adaptation\n",
        "\n",
        "Teaching TinyLlama theological knowledge from your 373K chunks.\n",
        "\n",
        "**Time**: ~8-12 hours on T4 GPU"
      ],
      "metadata": {
        "id": "phase1_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PHASE 1: DOMAIN ADAPTATION\")\n",
        "print(\"Teaching TinyLlama theological knowledge\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "# Load TinyLlama\n",
        "print(\"üì• Loading TinyLlama-1.1B-Chat-v1.0...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,  # QLoRA for efficiency\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded\")\n",
        "\n",
        "# Add LoRA adapters\n",
        "print(\"üîß Adding LoRA adapters...\")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=True,\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LoRA adapters added\")\n",
        "print()"
      ],
      "metadata": {
        "id": "phase1_load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format dataset for training\n",
        "print(\"üîÑ Formatting dataset...\")\n",
        "\n",
        "def formatting_func(examples):\n",
        "    \"\"\"Format chunks for domain adaptation\"\"\"\n",
        "    texts = []\n",
        "    for text in examples[\"text\"]:\n",
        "        formatted = f\"\"\"<|im_start|>system\n",
        "You are a theological research assistant trained on biblical and SDA content.<|im_end|>\n",
        "<|im_start|>text\n",
        "{text}<|im_end|>\"\"\"\n",
        "        texts.append(formatted)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = Dataset.from_list(domain_data)\n",
        "dataset = dataset.map(formatting_func, batched=True)\n",
        "\n",
        "print(f\"‚úÖ Dataset formatted: {len(dataset):,} examples\")\n",
        "print()"
      ],
      "metadata": {
        "id": "phase1_format"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Phase 1\n",
        "print(\"üöÄ Starting Phase 1 training...\")\n",
        "print(\"   This will take ~8-12 hours\")\n",
        "print(\"   You can close the browser - training will continue\")\n",
        "print(\"   (Colab will email you when done)\")\n",
        "print()\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        max_steps=2000,  # Adjust based on dataset size\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=10,\n",
        "        output_dir=\"/content/phase1_output\",\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(\"\")\n",
        "print(\"‚úÖ Phase 1 training complete!\")\n",
        "print(\"\")"
      ],
      "metadata": {
        "id": "phase1_train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Phase 1 model\n",
        "print(\"üíæ Saving Phase 1 model...\")\n",
        "\n",
        "model.save_pretrained(\"/content/tinyowl-phase1\")\n",
        "tokenizer.save_pretrained(\"/content/tinyowl-phase1\")\n",
        "\n",
        "print(\"‚úÖ Phase 1 model saved to /content/tinyowl-phase1\")\n",
        "print(\"\")\n",
        "print(\"üéâ PHASE 1 COMPLETE!\")\n",
        "print(\"   Next: Run Phase 2 cells below\")"
      ],
      "metadata": {
        "id": "phase1_save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## PHASE 2: Instruction Tuning\n",
        "\n",
        "Teaching TinyLlama to answer theological questions.\n",
        "\n",
        "**Time**: ~4-8 hours on T4 GPU"
      ],
      "metadata": {
        "id": "phase2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PHASE 2: INSTRUCTION TUNING\")\n",
        "print(\"Teaching TinyLlama to answer questions\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "# Load Phase 1 model\n",
        "print(\"üì• Loading Phase 1 model...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"/content/tinyowl-phase1\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Phase 1 model loaded\")\n",
        "print()"
      ],
      "metadata": {
        "id": "phase2_load"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format Q&A dataset\n",
        "print(\"üîÑ Formatting Q&A dataset...\")\n",
        "\n",
        "def formatting_func_qa(examples):\n",
        "    \"\"\"Format Q&A pairs for instruction tuning\"\"\"\n",
        "    texts = []\n",
        "    for instruction, output in zip(examples[\"instruction\"], examples[\"output\"]):\n",
        "        text = f\"\"\"<|im_start|>system\n",
        "You are TinyOwl, a theological research assistant trained on biblical Scripture, Spirit of Prophecy, and Strong's concordance. Answer questions accurately based on this knowledge.<|im_end|>\n",
        "<|im_start|>user\n",
        "{instruction}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "{output}<|im_end|>\"\"\"\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "qa_dataset = Dataset.from_list(qa_data)\n",
        "qa_dataset = qa_dataset.map(formatting_func_qa, batched=True)\n",
        "\n",
        "print(f\"‚úÖ Q&A dataset formatted: {len(qa_dataset):,} examples\")\n",
        "print()"
      ],
      "metadata": {
        "id": "phase2_format"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Phase 2\n",
        "print(\"üöÄ Starting Phase 2 training...\")\n",
        "print(\"   This will take ~4-8 hours\")\n",
        "print()\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=qa_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=2,\n",
        "        learning_rate=2e-5,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=10,\n",
        "        output_dir=\"/content/phase2_output\",\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(\"\")\n",
        "print(\"‚úÖ Phase 2 training complete!\")\n",
        "print(\"\")"
      ],
      "metadata": {
        "id": "phase2_train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final TinyOwl model\n",
        "print(\"üíæ Saving TinyOwl 1.0...\")\n",
        "\n",
        "model.save_pretrained(\"/content/tinyowl-1.0\")\n",
        "tokenizer.save_pretrained(\"/content/tinyowl-1.0\")\n",
        "\n",
        "print(\"‚úÖ TinyOwl 1.0 saved to /content/tinyowl-1.0\")\n",
        "print(\"\")\n",
        "print(\"=\"*60)\n",
        "print(\"üéâ TINYOWL 1.0 TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\")\n",
        "print(\"Next steps:\")\n",
        "print(\"1. Test the model (next cell)\")\n",
        "print(\"2. Download the model (instructions below)\")\n",
        "print(\"3. Quantize to GGUF for distribution\")"
      ],
      "metadata": {
        "id": "phase2_save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Test Your Model\n",
        "\n",
        "Try some theological questions!"
      ],
      "metadata": {
        "id": "test_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load final model for inference\n",
        "print(\"üì• Loading TinyOwl 1.0 for testing...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"/content/tinyowl-1.0\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)  # Enable inference mode\n",
        "\n",
        "print(\"‚úÖ Model ready for testing!\")\n",
        "print()\n",
        "\n",
        "# Test function\n",
        "def ask_tinyowl(question):\n",
        "    \"\"\"Ask TinyOwl a theological question\"\"\"\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "You are TinyOwl, a theological research assistant trained on biblical Scripture, Spirit of Prophecy, and Strong's concordance.<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "    \n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the assistant's response\n",
        "    answer = response.split(\"<|im_start|>assistant\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "# Test questions\n",
        "test_questions = [\n",
        "    \"Who was Aaron?\",\n",
        "    \"What does the Bible say about the Sabbath?\",\n",
        "    \"Explain the sanctuary service\",\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing TinyOwl...\")\n",
        "print()\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"Q: {question}\")\n",
        "    answer = ask_tinyowl(question)\n",
        "    print(f\"A: {answer}\")\n",
        "    print()\n",
        "    print(\"-\" * 60)\n",
        "    print()"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Download Your Trained Model\n",
        "\n",
        "Package and download TinyOwl 1.0 to your computer."
      ],
      "metadata": {
        "id": "download_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the model for download\n",
        "print(\"üì¶ Packaging TinyOwl 1.0 for download...\")\n",
        "\n",
        "!zip -r tinyowl-1.0.zip /content/tinyowl-1.0/\n",
        "\n",
        "print(\"‚úÖ Model packaged!\")\n",
        "print(\"\")\n",
        "print(\"üì• Download instructions:\")\n",
        "print(\"   1. Look at the files panel on the left\")\n",
        "print(\"   2. Find 'tinyowl-1.0.zip'\")\n",
        "print(\"   3. Right-click ‚Üí Download\")\n",
        "print(\"\")\n",
        "print(\"‚ö†Ô∏è  Note: Colab deletes files when session ends!\")\n",
        "print(\"   Download NOW before closing this notebook\")\n",
        "\n",
        "# Show file size\n",
        "import os\n",
        "size_mb = os.path.getsize(\"/content/tinyowl-1.0.zip\") / 1024**2\n",
        "print(f\"\")\n",
        "print(f\"üìä File size: {size_mb:.1f} MB\")"
      ],
      "metadata": {
        "id": "download_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üéâ You Did It!\n",
        "\n",
        "**TinyOwl 1.0 is trained!**\n",
        "\n",
        "### What You Accomplished:\n",
        "‚úÖ Trained TinyLlama on 373K theological chunks  \n",
        "‚úÖ Fine-tuned on thousands of Q&A pairs  \n",
        "‚úÖ Created a custom theological AI model  \n",
        "\n",
        "### Next Steps:\n",
        "1. **Download the model** (instructions above)\n",
        "2. **Quantize to GGUF** for efficient inference\n",
        "3. **Package with your chat app**\n",
        "4. **Distribute TinyOwl to the world!**\n",
        "\n",
        "---\n",
        "\n",
        "**The vision is complete. TinyOwl lives. ü¶â**"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}
