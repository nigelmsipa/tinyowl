# Models Configuration
# Defines embedding models and LLMs used in the TinyOwl pipeline

# Embedding Models
embeddings:
  default:
    name: "BAAI/bge-large-en-v1.5"
    provider: "sentence-transformers"
    dimensions: 1024
    description: "High-performance embedding model that often outperforms OpenAI"
    use_case: "general text chunks"
  
  alternative:
    name: "e5-base-v2"
    provider: "sentence-transformers" 
    dimensions: 768
    description: "Higher dimensional model for more nuanced embeddings"
    use_case: "when precision is critical"
    
  domain_specific:
    theology:
      name: "BAAI/bge-large-en-v1.5"  # BGE-large for theology domain
      provider: "sentence-transformers"

# Large Language Models
llms:
  rag:
    name: "gpt-3.5-turbo"  # Used for RAG responses
    provider: "openai"
    temperature: 0.2
    max_tokens: 500
    system_prompt: "You are an AI assistant with deep knowledge of theology. Answer questions accurately based on the provided context. Always cite your sources."
  
  baseline:
    name: "llama-2-7b"  # Local model for baseline evaluation
    provider: "local"
    temperature: 0.1
    max_tokens: 500
    model_path: "models/llama-2-7b-chat.gguf"  # Path to local GGUF model
    
  target:
    name: "TinyLlama-1.1B"  # Model to be fine-tuned
    provider: "local"
    description: "Compact model to be fine-tuned on theology corpus"
    base_model: "TinyLlama/TinyLlama-1.1B-intermediate-step-480k-1T"
    model_path: "models/tinyllama-1.1b.bin"

# Fine-tuning Configuration
fine_tuning:
  method: "LoRA"  # Parameter-Efficient Fine-Tuning
  parameters:
    r: 16  # LoRA attention dimension
    alpha: 32  # LoRA alpha parameter
    dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"
  
  training:
    batch_size: 4
    gradient_accumulation_steps: 4
    learning_rate: 3e-4
    epochs: 3
    warmup_ratio: 0.03
    
  evaluation:
    eval_steps: 200
    metrics:
      - "accuracy"
      - "perplexity"
      - "rouge"

# ChromaDB Configuration
vector_db:
  embedding_function: "sentence-transformers"  # Use sentence-transformers
  default_model: "BAAI/bge-large-en-v1.5"  # Default embedding model
  distance_function: "cosine"  # Use cosine similarity
  
  collections:
    # Legacy logical groupings (kept for backward compatibility)
    theology:
      name: "theology"
      description: "Legacy logical collection (not used by live DB)"
      embedding_model: "BAAI/bge-large-en-v1.5"

    theology_verses:
      name: "theology_verses"
      description: "Legacy verses collection (unused)"
      embedding_model: "BAAI/bge-large-en-v1.5"

    theology_pericopes:
      name: "theology_pericopes"
      description: "Legacy pericopes collection (unused)"
      embedding_model: "BAAI/bge-large-en-v1.5"

    theology_chapters:
      name: "theology_chapters"
      description: "Legacy chapters collection (unused)"
      embedding_model: "BAAI/bge-large-en-v1.5"

    # Live DB collections (actual names in vectordb)
    kjv_verses:
      name: "kjv_verses"
      description: "KJV Layer A: Single verse chunks"
      embedding_model: "BAAI/bge-large-en-v1.5"

    kjv_pericopes:
      name: "kjv_pericopes"
      description: "KJV Layer B: Pericope chunks (3-7 verses)"
      embedding_model: "BAAI/bge-large-en-v1.5"

    kjv_chapters:
      name: "kjv_chapters"
      description: "KJV Layer C: Chapter-level chunks"
      embedding_model: "BAAI/bge-large-en-v1.5"

    web_verses:
      name: "web_verses"
      description: "WEB Layer A: Single verse chunks"
      embedding_model: "BAAI/bge-large-en-v1.5"

    web_pericopes:
      name: "web_pericopes"
      description: "WEB Layer B: Pericope chunks (3-7 verses)"
      embedding_model: "BAAI/bge-large-en-v1.5"

    web_chapters:
      name: "web_chapters"
      description: "WEB Layer C: Chapter-level chunks"
      embedding_model: "BAAI/bge-large-en-v1.5"

    strongs_concordance_entries:
      name: "strongs_concordance_entries"
      description: "Strong's concordance word-verse entries"
      embedding_model: "BAAI/bge-large-en-v1.5"
